The torus is a surface of revolution generated by revolving a circle in three-dimensional space about an axis coplanar with the circle.
A torus can be defined parametrically by the equations x = (R + r*cos(v))*cos(u), y = (R + r*cos(v))*sin(u), z = r*sin(v).
The major radius R is the distance from the center of the tube to the center of the torus.
The minor radius r is the radius of the tube itself.
Attention mechanisms allow neural networks to focus on relevant parts of the input when producing output.
The transformer architecture uses self-attention to process sequences in parallel rather than sequentially.
Multi-head attention allows the model to jointly attend to information from different representation subspaces.
Position encoding is necessary because the attention mechanism has no inherent notion of token order.
Bidirectional processing allows information to flow in both directions through the sequence.
The eight-stream parallel architecture processes information through major loops, minor loops, spirals, and cross flows.
EMA compounding with learnable alpha values allows adaptive memory across transformer layers.
Cognitive coherence theory suggests that psychological well-being depends on comprehensibility, manageability, and meaningfulness.
The Sense of Coherence scale measures these three components as a unified construct.
Shared mental models help teams coordinate by providing common understanding of tasks and processes.
Curriculum learning starts with simple examples and progressively increases difficulty during training.
Dynamic batch sizing adjusts the number of samples per step based on gradient statistics.
Progressive model growth adds capacity during training rather than starting with the full model.
Layer-wise learning rates allow different parts of the network to learn at different speeds.
Gradient clipping prevents exploding gradients by scaling down large gradient norms.
Early stopping prevents overfitting by halting training when validation loss stops improving.
The Adam optimizer combines momentum and adaptive learning rates for efficient training.
Weight decay regularizes the model by penalizing large weight values during optimization.
Warmup gradually increases the learning rate at the start of training for stability.
Cross-entropy loss measures the difference between predicted and actual probability distributions.
Softmax converts logits into a probability distribution over the vocabulary.
Temperature scaling controls the sharpness of the probability distribution during sampling.
Top-k sampling restricts generation to the k most likely next tokens.
Top-p (nucleus) sampling restricts to the smallest set of tokens with cumulative probability p.
Greedy decoding always selects the most likely next token at each step.
Beam search maintains multiple candidate sequences to find better overall completions.
The vocabulary maps tokens to integer IDs for processing by the neural network.
Special tokens like BOS, EOS, and PAD have specific meanings in the sequence.
Tokenization breaks text into subword units that balance vocabulary size and coverage.
Embeddings map discrete tokens to continuous vector representations.
Layer normalization stabilizes training by normalizing activations within each layer.
Residual connections allow gradients to flow directly through the network.
The feed-forward network applies two linear transformations with a nonlinearity between them.
GELU activation provides smooth, non-monotonic activation for better gradient flow.
Dropout randomly zeroes activations during training for regularization.
The language modeling objective predicts the next token given the previous context.
Perplexity measures how well a language model predicts a sample of text.
Fine-tuning adapts a pretrained model to a specific downstream task.
Transfer learning leverages knowledge from one task to improve performance on another.
The attention pattern shows which input positions the model focuses on for each output.
Sparse attention reduces computational cost by limiting which positions can attend to each other.
Flash attention optimizes memory usage through tiling and recomputation strategies.
Rotary position embeddings encode position through rotation in the embedding space.
ALiBi adds position-dependent biases directly to attention scores.
The context window limits how much previous text the model can consider.
Sliding window attention maintains a fixed-size context that moves through the sequence.
Mixture of experts routes different inputs to specialized sub-networks.
Quantization reduces model size by using lower precision numerical representations.
Pruning removes unnecessary weights to create smaller, faster models.
Knowledge distillation trains a smaller student model to match a larger teacher.
The training loop iterates over batches, computing loss and updating weights.
Checkpointing saves model state to allow resuming interrupted training.
Gradient accumulation simulates larger batches by summing gradients across steps.
Mixed precision training uses lower precision for speed while maintaining accuracy.
Distributed training splits work across multiple devices for faster processing.
Data parallelism replicates the model and splits the data across devices.
Model parallelism splits the model itself across devices for very large models.
Pipeline parallelism processes different layers on different devices in sequence.
The learning rate schedule adjusts the step size over the course of training.
Cosine annealing smoothly decreases the learning rate following a cosine curve.
Linear warmup increases the learning rate linearly during the initial training phase.
Step decay reduces the learning rate by a factor at specified training steps.
The validation set estimates generalization performance during training.
The test set provides final evaluation on held-out data after training completes.
Overfitting occurs when the model memorizes training data instead of learning patterns.
Underfitting occurs when the model is too simple to capture the data's structure.
The bias-variance tradeoff balances model complexity against generalization ability.
Regularization techniques prevent overfitting by constraining the model's capacity.
Data augmentation increases effective dataset size by creating modified copies of examples.
Batch normalization normalizes across the batch dimension for training stability.
The loss landscape describes how the loss function varies with model parameters.
Local minima are points where the loss is lower than nearby points but not globally.
Saddle points have zero gradient but are not minima in all directions.
Momentum helps optimization escape shallow local minima and saddle points.
The Hessian matrix contains second derivatives and describes local curvature.
Natural gradient descent accounts for the geometry of the parameter space.
Second-order methods use curvature information for more efficient optimization.
The Fisher information matrix approximates the Hessian for probabilistic models.
Gradient noise from mini-batches can help escape sharp minima.
Flat minima may generalize better than sharp minima according to some theories.
The loss surface becomes increasingly non-convex as model depth increases.
Skip connections help gradient flow and enable training of very deep networks.
Batch size affects both training dynamics and final model performance.
Large batches train faster but may converge to sharper minima.
Small batches provide regularization through gradient noise.
The critical batch size balances these tradeoffs for a given problem.
Learning rate and batch size are often scaled together during training.
Hyperparameter tuning searches for the best configuration of training settings.
Grid search evaluates all combinations of hyperparameter values.
Random search samples hyperparameters randomly from specified distributions.
Bayesian optimization models the objective function to guide the search.
Population-based training evolves hyperparameters during training.
Neural architecture search automates the design of network structures.
AutoML systems automate multiple aspects of the machine learning pipeline.
