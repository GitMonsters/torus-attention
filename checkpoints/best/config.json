{
  "config": {
    "vocab_size": 50257,
    "max_seq_len": 256,
    "hidden_dim": 64,
    "num_layers": 2,
    "num_heads": 4,
    "ffn_dim": 256,
    "dropout": 0.1,
    "layer_norm_eps": 0.00001,
    "tie_embeddings": true,
    "torus_major_radius": 2.0,
    "torus_minor_radius": 1.0,
    "ema_alpha": 0.9,
    "use_coherence": true
  },
  "step": 175,
  "loss": 33.62616729736328,
  "timestamp": "1768894270",
  "version": "0.1.0"
}